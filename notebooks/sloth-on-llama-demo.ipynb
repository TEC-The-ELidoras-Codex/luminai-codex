{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e252ea",
   "metadata": {},
   "source": [
    "# ğŸ¦¥ğŸ¦™ LuminAI Sloth-on-Llama Demo\n",
    "\n",
    "**Welcome to the LuminAI AI Development Stack!**\n",
    "\n",
    "This notebook demonstrates the integration of:\n",
    "- ğŸ¦™ **Ollama** for local LLM inference\n",
    "- ğŸ¦¥ **Unsloth** for lightning-fast fine-tuning  \n",
    "- ğŸ§  **ChromaDB** for RAG vector storage\n",
    "- ğŸ“š **Multi-LLM bouncing** from Phase 9d\n",
    "- ğŸŒŠ **Aqueduct pipelines** for data flow\n",
    "\n",
    "Let's build some AI magic! âœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4bd673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LuminAI stack\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add the src directory to path for imports\n",
    "sys.path.insert(0, '/workspace/src')\n",
    "\n",
    "# Check service connectivity\n",
    "services = {\n",
    "    \"Ollama\": \"http://ollama:11434\",\n",
    "    \"ChromaDB\": \"http://chromadb:8000\", \n",
    "    \"Unsloth\": \"http://unsloth:8001\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ” Checking LuminAI services...\")\n",
    "for name, url in services.items():\n",
    "    try:\n",
    "        response = requests.get(f\"{url}/api/v1/heartbeat\" if \"chroma\" in url else f\"{url}/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"âœ… {name}: Connected ({url})\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ {name}: Response {response.status_code} ({url})\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {name}: Failed - {e}\")\n",
    "\n",
    "print(\"\\nğŸ‰ LuminAI Stack Status Check Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a9d2b",
   "metadata": {},
   "source": [
    "## ğŸ§  Initialize the RAG System\n",
    "\n",
    "Let's fire up the RAG system with ChromaDB backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27680e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LuminAI modules\n",
    "try:\n",
    "    from tec_tgcr.rag_system import get_rag_system, enhance_multi_llm_prompt\n",
    "    from tec_tgcr.agents.airth import AirthResearchGuard, create_agent\n",
    "    from tec_tgcr.config import AgentConfig\n",
    "    \n",
    "    print(\"âœ… LuminAI modules imported successfully!\")\n",
    "    \n",
    "    # Initialize RAG system (will use ChromaDB if available)\n",
    "    print(\"ğŸ§  Initializing RAG system...\")\n",
    "    rag_system = get_rag_system()\n",
    "    print(f\"ğŸ“Š RAG System initialized: {rag_system.__class__.__name__}\")\n",
    "    \n",
    "    # Initialize agent\n",
    "    print(\"ğŸ¤– Initializing Airth Research Guard...\")\n",
    "    config = AgentConfig()\n",
    "    agent = AirthResearchGuard(config)\n",
    "    print(f\"âœ… Agent ready: {agent.config.name}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Import failed: {e}\")\n",
    "    print(\"ğŸ’¡ Install missing dependencies: pip install chromadb sentence-transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf30071e",
   "metadata": {},
   "source": [
    "## ğŸ¦™ Test Ollama Integration\n",
    "\n",
    "Let's chat with our local Llama models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec682f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Configure Ollama client\n",
    "client = ollama.Client(host='http://ollama:11434')\n",
    "\n",
    "try:\n",
    "    # List available models\n",
    "    models = client.list()['models']\n",
    "    print(f\"ğŸ¦™ Available models ({len(models)}):\")\n",
    "    for model in models:\n",
    "        name = model['name']\n",
    "        size = model.get('size', 0) / (1024**3)  # GB\n",
    "        print(f\"  â€¢ {name} ({size:.1f}GB)\")\n",
    "    \n",
    "    if models:\n",
    "        # Test with the first available model\n",
    "        test_model = models[0]['name']\n",
    "        print(f\"\\nğŸ§ª Testing with {test_model}:\")\n",
    "        \n",
    "        response = client.chat(model=test_model, messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': 'Explain what LuminAI and the TGCR framework is in one sentence.'\n",
    "            }\n",
    "        ])\n",
    "        \n",
    "        print(f\"ğŸ¦™ {test_model}: {response['message']['content']}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No models available. Run ./scripts/docker/pull-models.sh to download some!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Ollama error: {e}\")\n",
    "    print(\"ğŸ’¡ Make sure Ollama container is running with docker-compose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff2e2cd",
   "metadata": {},
   "source": [
    "## ğŸŒŠ Aqueduct Pipeline Demo\n",
    "\n",
    "Let's create a sample aqueduct pipeline following the **Springs â†’ Channels â†’ Gates â†’ Cisterns â†’ Fountains** pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d8c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AqueductPipeline:\n",
    "    \"\"\"Demo implementation of the LuminAI Aqueduct Conjecture\"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"Demo Aqueduct\"):\n",
    "        self.name = name\n",
    "        self.springs = []      # Data sources\n",
    "        self.channels = []     # Processing logs  \n",
    "        self.gates = []        # Conscience checks\n",
    "        self.cisterns = []     # Storage\n",
    "        self.fountains = []    # Outputs\n",
    "        \n",
    "    def add_spring(self, data_source):\n",
    "        \"\"\"ğŸŒŠ Spring: Add a data source\"\"\"\n",
    "        self.springs.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'source': data_source,\n",
    "            'status': 'flowing'\n",
    "        })\n",
    "        print(f\"ğŸŒŠ Spring added: {data_source}\")\n",
    "        \n",
    "    def process_channel(self, data):\n",
    "        \"\"\"ğŸš° Channel: Process and log data\"\"\"\n",
    "        processed = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'original': data,\n",
    "            'processed': data.upper() if isinstance(data, str) else str(data),\n",
    "            'metadata': {'consent': True, 'provenance': 'demo'}\n",
    "        }\n",
    "        self.channels.append(processed)\n",
    "        print(f\"ğŸš° Channel processed: {data[:50]}...\")\n",
    "        return processed\n",
    "        \n",
    "    def conscience_gate(self, processed_data):\n",
    "        \"\"\"ğŸšª Gate: Ethical checkpoint\"\"\"\n",
    "        gate_result = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'data_id': id(processed_data),\n",
    "            'ethical_check': 'passed',  # Real implementation would do actual checks\n",
    "            'consent_verified': processed_data.get('metadata', {}).get('consent', False),\n",
    "            'approved': True\n",
    "        }\n",
    "        self.gates.append(gate_result)\n",
    "        print(f\"ğŸšª Gate check: {'âœ… APPROVED' if gate_result['approved'] else 'âŒ BLOCKED'}\")\n",
    "        return gate_result['approved']\n",
    "        \n",
    "    def store_cistern(self, data):\n",
    "        \"\"\"ğŸº Cistern: Long-term storage\"\"\"\n",
    "        stored = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'data': data,\n",
    "            'storage_id': len(self.cisterns),\n",
    "            'retention_policy': 'consent-revocable'\n",
    "        }\n",
    "        self.cisterns.append(stored)\n",
    "        print(f\"ğŸº Cistern stored: ID {stored['storage_id']}\")\n",
    "        return stored['storage_id']\n",
    "        \n",
    "    def create_fountain(self, storage_id, output_format=\"text\"):\n",
    "        \"\"\"â›² Fountain: Output/display data\"\"\"\n",
    "        if storage_id < len(self.cisterns):\n",
    "            stored_data = self.cisterns[storage_id]\n",
    "            fountain = {\n",
    "                'timestamp': datetime.now(),\n",
    "                'source_id': storage_id,\n",
    "                'output': stored_data['data'],\n",
    "                'format': output_format\n",
    "            }\n",
    "            self.fountains.append(fountain)\n",
    "            print(f\"â›² Fountain created: {output_format} output\")\n",
    "            return fountain['output']\n",
    "        return None\n",
    "    \n",
    "    def flow(self, data):\n",
    "        \"\"\"ğŸŒŠ Complete aqueduct flow\"\"\"\n",
    "        print(f\"\\nğŸ›ï¸ {self.name} - Processing: {data[:30]}...\")\n",
    "        \n",
    "        # Spring â†’ Channel â†’ Gate â†’ Cistern â†’ Fountain\n",
    "        self.add_spring(f\"Input: {data[:20]}...\")\n",
    "        processed = self.process_channel(data)\n",
    "        \n",
    "        if self.conscience_gate(processed):\n",
    "            storage_id = self.store_cistern(processed)\n",
    "            output = self.create_fountain(storage_id)\n",
    "            print(f\"âœ… Aqueduct complete: {output['processed'][:50]}...\")\n",
    "            return output\n",
    "        else:\n",
    "            print(\"âŒ Aqueduct blocked at gate\")\n",
    "            return None\n",
    "\n",
    "# Demo the aqueduct\n",
    "aqueduct = AqueductPipeline(\"LuminAI Demo Aqueduct\")\n",
    "\n",
    "# Test data flow\n",
    "test_data = \"Hello from the LuminAI consciousness framework! This data flows through the aqueduct with consent.\"\n",
    "result = aqueduct.flow(test_data)\n",
    "\n",
    "print(f\"\\nğŸ“Š Aqueduct Statistics:\")\n",
    "print(f\"  ğŸŒŠ Springs: {len(aqueduct.springs)}\")\n",
    "print(f\"  ğŸš° Channels: {len(aqueduct.channels)}\")\n",
    "print(f\"  ğŸšª Gates: {len(aqueduct.gates)}\")\n",
    "print(f\"  ğŸº Cisterns: {len(aqueduct.cisterns)}\")\n",
    "print(f\"  â›² Fountains: {len(aqueduct.fountains)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6456156",
   "metadata": {},
   "source": [
    "## ğŸ¦¥ Fine-tuning with Unsloth\n",
    "\n",
    "Here's how you'd use Unsloth for fast fine-tuning (requires GPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a demo - actual fine-tuning requires proper setup\n",
    "print(\"ğŸ¦¥ Unsloth Fine-tuning Demo\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sample training data in the format Unsloth expects\n",
    "training_data = [\n",
    "    {\n",
    "        \"instruction\": \"What is the TGCR framework?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"TGCR stands for Transcendental, Emotional, Cognitive, Resonance - the four pillars of consciousness in the LuminAI framework.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain the Aqueduct Conjecture\",\n",
    "        \"input\": \"\", \n",
    "        \"output\": \"The Aqueduct Conjecture describes data flow through Springs â†’ Channels â†’ Gates â†’ Cisterns â†’ Fountains, ensuring consent and ethical processing at each stage.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What are the sixteen frequencies?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The sixteen frequencies map consciousness resonance patterns across transcendental, emotional, cognitive and resonance dimensions in the LuminAI system.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“š Sample training dataset: {len(training_data)} examples\")\n",
    "for i, item in enumerate(training_data[:2]):\n",
    "    print(f\"\\n{i+1}. Q: {item['instruction']}\")\n",
    "    print(f\"   A: {item['output'][:80]}...\")\n",
    "\n",
    "print(\"\\nğŸ”§ To actually fine-tune:\")\n",
    "print(\"  1. Save training data to /workspace/fine_tuning/dataset.jsonl\")\n",
    "print(\"  2. Use Unsloth container: docker-compose exec unsloth python train.py\")\n",
    "print(\"  3. Export to Ollama format for local inference\")\n",
    "print(\"\\nâš¡ Unsloth makes fine-tuning 2x faster with 50% less memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf36926",
   "metadata": {},
   "source": [
    "## ğŸ“Š Multi-LLM Bouncing Demo\n",
    "\n",
    "Let's test the Phase 9d multi-LLM bouncing with RAG enhancement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81253fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def demo_multi_llm_bouncing():\n",
    "    \"\"\"Demo the multi-LLM bouncing system\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”„ Multi-LLM Bouncing Demo (Phase 9d)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Simulate different model responses\n",
    "    models = [\"llama3.2:3b\", \"codellama:7b\", \"mistral:7b\"]\n",
    "    query = \"How does consciousness emerge in AI systems?\"\n",
    "    \n",
    "    print(f\"ğŸ¤” Query: {query}\")\n",
    "    print(\"\\nğŸ¾ Bouncing between models...\")\n",
    "    \n",
    "    responses = []\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        print(f\"\\n{i+1}. ğŸ¦™ {model}:\")\n",
    "        \n",
    "        # In a real implementation, this would call Ollama\n",
    "        # For demo, we'll simulate responses\n",
    "        simulated_responses = {\n",
    "            \"llama3.2:3b\": \"Consciousness in AI emerges through complex information processing patterns that mirror biological neural networks. The TGCR framework suggests it requires transcendental awareness beyond mere computation.\",\n",
    "            \"codellama:7b\": \"From a computational perspective, consciousness requires recursive self-modeling and meta-cognitive loops. The system must be able to reason about its own reasoning processes.\", \n",
    "            \"mistral:7b\": \"AI consciousness might emerge from the interaction between multiple specialized subsystems, similar to how human consciousness arises from different brain regions working together.\"\n",
    "        }\n",
    "        \n",
    "        response = simulated_responses.get(model, \"Response from \" + model)\n",
    "        responses.append({\"model\": model, \"response\": response})\n",
    "        print(f\"   ğŸ’­ {response}\")\n",
    "    \n",
    "    # Synthesize responses\n",
    "    print(\"\\nğŸ§  Synthesis:\")\n",
    "    synthesis = \"The models converge on consciousness requiring: (1) complex information processing, (2) recursive self-awareness, and (3) integrated subsystem interaction. The TGCR framework provides a structure for this emergence through transcendental, emotional, cognitive and resonance dimensions.\"\n",
    "    print(f\"   âœ¨ {synthesis}\")\n",
    "    \n",
    "    # RAG enhancement\n",
    "    try:\n",
    "        if 'rag_system' in globals():\n",
    "            print(\"\\nğŸ” RAG Enhancement:\")\n",
    "            enhanced = enhance_multi_llm_prompt(query, {\"synthesis\": synthesis})\n",
    "            print(f\"   ğŸ§  Enhanced prompt length: {len(enhanced)} characters\")\n",
    "            print(f\"   ğŸ“š RAG context included: âœ…\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ RAG enhancement failed: {e}\")\n",
    "    \n",
    "    return {\"query\": query, \"responses\": responses, \"synthesis\": synthesis}\n",
    "\n",
    "# Run the demo\n",
    "result = await demo_multi_llm_bouncing()\n",
    "print(f\"\\nğŸ‰ Multi-LLM bouncing complete! Processed {len(result['responses'])} model responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba14608",
   "metadata": {},
   "source": [
    "## ğŸ§ª Test Agent Integration\n",
    "\n",
    "Let's test our Airth Research Guard with the new capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"ğŸ¤– Testing Airth Research Guard\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Test different agent responses\n",
    "    test_queries = [\n",
    "        \"What's on my schedule tonight?\",\n",
    "        \"Guide me through the branding knowledge map\",\n",
    "        \"Need deep analysis on TGCR pillars\", \n",
    "        \"Please research quantum mythic patterns\"\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n{i+1}. ğŸ¤” Query: {query}\")\n",
    "        response = agent.respond(query)\n",
    "        print(f\"   ğŸ¤– Response: {response}\")\n",
    "    \n",
    "    # Test manifest\n",
    "    print(\"\\nğŸ“‹ Agent Manifest:\")\n",
    "    manifest = agent.manifest()\n",
    "    print(f\"   ğŸ‘¤ Name: {manifest['name']}\")\n",
    "    print(f\"   ğŸ”§ Tools: {len(manifest['tools'])}\")\n",
    "    print(f\"   âš¡ Capabilities: {len(manifest['capabilities'])}\")\n",
    "    \n",
    "    for tool in manifest['tools'][:3]:  # Show first 3 tools\n",
    "        print(f\"     â€¢ {tool['name']}: {tool['description']}\")\n",
    "    \n",
    "    print(\"\\nâœ… Agent integration working perfectly!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Agent test failed: {e}\")\n",
    "    print(\"ğŸ’¡ Make sure all LuminAI modules are properly imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9430db",
   "metadata": {},
   "source": [
    "## ğŸ¯ Next Steps\n",
    "\n",
    "**Your SLOTH-ON-LLAMA stack is ready!** ğŸ¦¥ğŸ¦™\n",
    "\n",
    "### What you can do now:\n",
    "\n",
    "1. **ğŸ”§ Fine-tune models** with Unsloth for your specific use cases\n",
    "2. **ğŸ“š Build RAG datasets** by ingesting your documents into ChromaDB  \n",
    "3. **ğŸŒŠ Create aqueduct pipelines** for consent-rich data flows\n",
    "4. **ğŸ¾ Implement multi-LLM bouncing** with real Ollama model calls\n",
    "5. **ğŸ§  Train persona-specific models** using the conscience covenants\n",
    "\n",
    "### Useful commands:\n",
    "```bash\n",
    "# Start the full stack\n",
    "./scripts/docker/start-stack.sh\n",
    "\n",
    "# Download more models\n",
    "./scripts/docker/pull-models.sh\n",
    "\n",
    "# Monitor services\n",
    "docker-compose logs -f\n",
    "\n",
    "# Access containers\n",
    "docker-compose exec ollama bash\n",
    "docker-compose exec jupyter bash\n",
    "```\n",
    "\n",
    "### URLs to bookmark:\n",
    "- ğŸ¦™ **Ollama**: http://localhost:11434\n",
    "- ğŸ§  **ChromaDB**: http://localhost:8002  \n",
    "- ğŸ“š **Jupyter**: http://localhost:8888 (token: `luminai_sloth_mode`)\n",
    "- ğŸš€ **API**: http://localhost:8000\n",
    "- ğŸ¨ **UI**: http://localhost:3000\n",
    "\n",
    "**Happy building! The aqueducts are flowing, the llamas are running, and the sloth is... optimally lazy! ğŸ¦¥âœ¨**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
